================================================================================================================
============= Reading data from bq table =======================================================================
================================================================================================================

from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Read from BigQuery") \
    .config("spark.jars", "gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar") \
    .getOrCreate()

# Set up the BigQuery connector .. This is optional
spark.conf.set("spark.sql.extensions", "com.google.cloud.spark.bigquery.BigQueryRelationProvider")

# Define BigQuery table to read
table_name = "your_project.your_dataset.your_table"  # Replace with your project ID, dataset, and table name

# Read data from BigQuery into a DataFrame
df = spark.read.format("bigquery") \
    .option("table", table_name) \
    .load()

# Show the first few rows of the DataFrame
df.show()

# Stop Spark session
spark.stop()

================================================================================================================================================================
## If The spark-bigquery connector is included in the Spark distribution used on Google Cloud Dataproc and other managed Spark environments by default. 
##Then no need to connect bq connector using spark.conf.set

from pyspark.sql import SparkSession
# Initialize Spark session
spark = SparkSession.builder \
    .appName("Read from BigQuery") \
    .getOrCreate()

# Define BigQuery table to read
table_name = "your_project.your_dataset.your_table"  # Replace with your project ID, dataset, and table name

# Read data from BigQuery into a DataFrame
df = spark.read.format("bigquery") \
    .option("table", table_name) \
    .load()

# Show the first few rows of the DataFrame
df.show()

# Stop Spark session
spark.stop()




================================================================================================================
============= Reading only the selected columns from bq ========================================================
================================================================================================================

from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Read selected columns with SQL from BigQuery") \
    .getOrCreate()

# Define BigQuery SQL query to execute
sql_query = """
    SELECT column1, column2, column3
    FROM your_project.your_dataset.your_table
    WHERE conditions_here
"""

# Read data from BigQuery into a DataFrame, executing the SQL query
df = spark.read.format("bigquery") \
    .option("query", sql_query) \
    .load()

# Show the first few rows of the DataFrame
df.show()

# Stop Spark session
spark.stop()




================================================================================================================
============= Inserting data into BQ table from flat file ======================================================
================================================================================================================
## Inserting into tables from a flat file:-

#--create spark session. 
#-- create a dataframe:-
df = spark.read.option("delimiter", "|").csv(data_path)
-- insert into table:-
df.write \ 
	.format("bigquery") \ 
	.option("temporaryGcsBucket", "your-temporary-gcs-bucket") \ 
	.option("table", f"{project_id}.{dataset_id}.{table_name}") \ 
	.save()
-------------------------------------------------------------
# Example:-
from pyspark.sql import SparkSession
# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Insert into BigQuery table from .DAT file") \
    .getOrCreate()

# Set GCS bucket and file path
bucket_name = "your_bucket_name"
file_name = "path/to/your/file.dat"

# Read .DAT file into a DataFrame
df = spark.read.option("delimiter", "|").csv(f"gs://{bucket_name}/{file_name}")

# Define BigQuery dataset and table names
dataset_name = "your_dataset_name"
table_name = "your_table_name"

# Write DataFrame to BigQuery
df.write \
    .format("bigquery") \
    .option("table", f"{dataset_name}.{table_name}") \
    .mode("append") \
    .save()

# Stop SparkSession
spark.stop()


===========================================================================
Note: In PySpark, the spark-big query connector can directly interact with BigQuery without needing to import bigquery and storage modules explicitly, unlike in regular Python code where you need to import these modules from the Google Cloud client libraries.
spark-bigquery connector is not explicitly mentioned in the provided PySpark code. The PySpark code assumes that you have already configured the spark-bigquery connector in your Spark environment.
Set the necessary Spark configurations to include the spark-bigquery connector JAR file in your Spark application's classpath. You can do this using the spark.jars configuration property when creating your SparkSession.
spark = SparkSession.builder \ 
	.appName("Read from BigQuery with PySpark") \ 
	.config("spark.jars", "/path/to/spark-bigquery-connector.jar") \
	.getOrCreate()
===========================================================================


